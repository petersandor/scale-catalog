image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4@sha256:06547ed7c5c723cc6f0d0c4b650cc4f75cfecf7e078feca41a1ace42bd8ca442
ffmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-ffmpeg-core@sha256:37421b1603468ee95675906db50e2538fcf8ae1b65089d0262c437fd78c6edd0
cublasCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-cublas-cuda12-core@sha256:76fee9cabe37e435c6eb849d144980440a546906e76afed784c43ce21029aa17
cublasCuda12FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-cublas-cuda12-ffmpeg-core@sha256:c08cf2b08364c15894b04b0ca483781ced8e18c7a09e29c2a94b848b44c57e60
cublasCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-cublas-cuda11-core@sha256:556feef18be107a6ebb99d0eda0d9d5d841eeafc3e5de5a90b484d18c8a7b2b2
cublasCuda11FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-cublas-cuda11-ffmpeg-core@sha256:98603a0522024d5bcdd23e17474c3fbf6f841e252801b1ad9d123a77fbed2f5a
allInOneCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-aio-gpu-nvidia-cuda-12@sha256:1839882834904decac4cba73e7641dd0849d348fee3318ad3d3ff29ba478b977
allInOneCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-aio-gpu-nvidia-cuda-11@sha256:75504a6b5825675dabb948794a6015a2d37c1eec73a54c10ef439c87d0857c98
allInOneCpuImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.4-aio-cpu@sha256:29f0d3bbb1803b79f64343137d8e00cec09192d0f9e73dd6169c407bf84afdb7
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0
service:
  main:
    ports:
      main:
        protocol: http
        port: 8080
localai:
  # Specify a build type. Available: cublas, openblas, clblas.
  build_type: "openblas"
  debug: false
  cors: true
  cors_allow_origins: "*"
  galleries: []
  #  - name: model-gallery
  #    url: github:go-skynet/model-gallery/index.yaml
  preload_models: []
  #    url: github:go-skynet/model-gallery/gpt4all-j.yaml
  # UPLOAD_LIMIT
workload:
  main:
    podSpec:
      containers:
        main:
          probes:
            liveness:
              enabled: true
              type: http
              path: /readyz
            readiness:
              enabled: true
              type: http
              path: /readyz
            startup:
              enabled: true
              type: tcp
          imageSelector: image
          env:
            ADDRESS: ":{{ .Values.service.main.ports.main.port }}"
            MODELS_PATH: "{{ .Values.persistence.models.mountPath }}"
            IMAGE_PATH: "{{ .Values.persistence.images.mountPath }}"
            BUILD_TYPE: "{{ .Values.localai.build_type }}"
            # breaks chart if true, keep it false.
            REBUILD: false
            DEBUG: "{{ .Values.localai.debug }}"
            CORS: "{{ .Values.localai.cors }}"
            GALLERIES: "{{ toJson .Values.localai.galleries }}"
            PRELOAD_MODELS: "{{ toJson .Values.localai.preload_models }}"
            CORS_ALLOW_ORIGINS: "{{ .Values.localai.cors_allow_origins }}"
persistence:
  models:
    enabled: true
    mountPath: "/models"
  images:
    enabled: true
    mountPath: "/images"
portal:
  open:
    enabled: false
updated: true
